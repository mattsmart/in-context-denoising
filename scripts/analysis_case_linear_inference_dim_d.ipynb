{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Dynamically find the path to `src/`\n",
    "notebook_dir = os.getcwd()  # Get current working directory\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, \"..\"))  # Go up one level\n",
    "src_path = os.path.join(project_root, \"src\")\n",
    "\n",
    "# Add to sys.path if not already present\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)"
   ],
   "id": "264a371a447993bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.data_io import load_runinfo_from_rundir, reload_lossbaseline_dict\n",
    "from src.data_tools import data_train_test_split_linear, DatasetWrapper, load_dataset\n",
    "from src.nn_loss_baselines import theory_linear_expected_error, report_dataset_loss\n",
    "from src.nn_model_base import load_modeltype_from_fname, load_model_from_rundir\n",
    "from src.nn_train_methods import (train_model, theory_linear_expected_error, loss_if_predict_linalg,\n",
    "                              loss_if_predict_linalg_shrunken, loss_if_predict_zero)\n",
    "from src.settings import DIR_MODELS, DIR_OUT, DIR_RUNS, COLOR_TARGET, COLOR_PRED, COLOR_INPUT"
   ],
   "id": "cf3bf0e9e3e377ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Script for analysis of case 0: linear subspace task (with varying context length)\n",
    "\"\"\"\n",
    "\n",
    "def gamma_star(sigma2_pure_context, sigma2_corruption):\n",
    "    return 1 / (sigma2_pure_context + sigma2_corruption)\n",
    "\n",
    "\n",
    "COLOR_TRAIN      = '#4C72B0'\n",
    "COLOR_TEST       = '#7BC475'\n",
    "COLOR_TEST_DARK1 = '#64A461'\n",
    "COLOR_TEST_DARK2 = '#569256'\n",
    "COLOR_PROJ       = '#BCBEC0'\n",
    "COLOR_PROJSHRUNK = '#B56BAC'"
   ],
   "id": "9d91cb1914634939",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train a model on the linear manifold task\n",
    "(could alternatively load a model)"
   ],
   "id": "c3136302ebf48443"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# these determine both (A) dataset generation and (B) the model currently\n",
    "context_len = 500   # this will induce the synthetic dataset via data_train_test_split_linear  - 500\n",
    "dim_n = 16           # this will induce the synthetic dataset ^^ (try 16/32 to 128)\n",
    "\n",
    "nn_model = 'TransformerModelV1noresOmitLast'\n",
    "\n",
    "datagen_case = 0  # {0, 1, 2} -> {linear, GMM clusters, manifold}\n",
    "\n",
    "seed_dataset = 0  # works here\n",
    "seed_torch   = 0  # dataset 4 with seed 4 is problem for multitraj 100/16/8 gradflow sgd 0.5\n",
    "\n",
    "epochs = 100\n",
    "# defaults:\n",
    "# - if train size is 8000, set batch_size to 800\n",
    "# - if train size is 800,  set batch_size to 80\n",
    "batch_size = 80  # reminder: 'manifold' notebook is using batch size of one (1)\n",
    "\n",
    "# (in num. batches - default of 4 when bsz was 80, 800 size train set, so 10 batches per epoch)\n",
    "#   e.g. 1000 batch-per-epoch, then get 10 loss evals per epoch for full_loss_sample_interval = 100\n",
    "full_loss_sample_interval = 4\n",
    "\n",
    "optimizer_choice = 'adam'  # sgd or adam\n",
    "\n",
    "if optimizer_choice == 'sgd':\n",
    "    optimizer_lr = 0.1  #0.01  # 0.5\n",
    "    #optimizer_lr = 80*0.1  #1.  400 epochs for spheres case with original params\n",
    "    # scheduler_kwargs = None\n",
    "    scheduler_kwargs = dict(milestones=[0.8*int(epochs), 0.9*int(epochs)], gamma=0.1)  # mult by gamma each milestone\n",
    "else:\n",
    "    assert optimizer_choice == 'adam'\n",
    "    optimizer_lr = 1e-2  # default: 1e-2\n",
    "    scheduler_kwargs = None\n",
    "\n",
    "flag_save_dataset   = True  # default: False (filesize can be large)\n",
    "flag_vis_loss       = True\n",
    "flag_vis_weights    = True\n",
    "flag_vis_batch_perf = True\n",
    "\n",
    "# prep dataset generation kwargs\n",
    "# ################################################################################\n",
    "num_W_in_dataset = 1000\n",
    "context_examples_per_W = 1\n",
    "samples_per_context_example = 1\n",
    "\n",
    "base_kwargs = dict(\n",
    "    context_len=context_len,\n",
    "    dim_n=dim_n,\n",
    "    num_W_in_dataset=num_W_in_dataset,  # this will be train_plus_test_size (since other kwargs are 1)\n",
    "    context_examples_per_W=context_examples_per_W,\n",
    "    samples_per_context_example=samples_per_context_example,\n",
    "    test_ratio=0.2,\n",
    "    verbose=True,\n",
    "    as_torch=True,\n",
    "    savez_fname=None,  # will be set internally depending on flag_save_dataset\n",
    "    seed=seed_dataset,\n",
    "    style_origin_subspace=True,\n",
    "    style_corruption_orthog=False,\n",
    ")\n",
    "\n",
    "# manually modify these case-specific settings if desired\n",
    "assert datagen_case == 0\n",
    "linear_datagen_kwargs = base_kwargs | dict(\n",
    "    sigma2_corruption=1.0,\n",
    "    sigma2_pure_context=2.0,\n",
    "    style_subspace_dimensions=8,  # int or 'random'\n",
    ")\n",
    "\n",
    "(net, model_fname, io_dict, loss_vals_dict,\n",
    " train_loader, test_loader, x_train, y_train, x_test, y_test,\n",
    " train_data_subspaces, test_data_subspaces) = train_model(\n",
    "    nn_model=nn_model,\n",
    "    restart_nn_instance=None,\n",
    "    restart_dataset=None,\n",
    "    train_plus_test_size=num_W_in_dataset * context_examples_per_W * samples_per_context_example,\n",
    "    context_len=context_len, dim_n=dim_n,\n",
    "    datagen_case=datagen_case,\n",
    "    datagen_kwargs=linear_datagen_kwargs,\n",
    "    batch_size=batch_size,\n",
    "    seed_torch=seed_torch,\n",
    "    epochs=epochs,\n",
    "    optimizer_choice=optimizer_choice, optimizer_lr=optimizer_lr, scheduler_kwargs=scheduler_kwargs,\n",
    "    full_loss_sample_interval=full_loss_sample_interval,\n",
    "    flag_save_dataset=flag_save_dataset,\n",
    "    flag_vis_loss=flag_vis_loss,\n",
    "    flag_vis_weights=flag_vis_weights,\n",
    "    flag_vis_batch_perf=flag_vis_batch_perf)"
   ],
   "id": "e2878f607f8e7a06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Main data storage object\n",
    "datadict_eval_loss = dict()  # will fill up one for each evaluated dim d (plus one for 'train' baselines\n",
    "datadict_eval_loss['linalg_proj'] = dict()\n",
    "datadict_eval_loss['linalg_proj_shrunken'] = dict()\n",
    "datadict_eval_loss['predict_zero'] = dict()\n",
    "\n",
    "# 1) Load the model we just trained\n",
    "models_to_load = [ io_dict['dir_base'] ]\n",
    "model_dirs     = [ a for a in models_to_load] \n",
    "model_labels   = ['n=16, d=8']\n",
    "\n",
    "\"\"\"\n",
    "# 1) Load a previously trained model\n",
    "# Could alternatively load a pre-existing training run\n",
    "dir_parent = DIR_RUNS\n",
    "models_to_load = ['check_ddim_vary_inference']\n",
    "model_dirs = [dir_parent + os.sep + a for a in models_to_load]\n",
    "model_labels = ['n=16, d=8']\n",
    "\"\"\"\n",
    "\n",
    "epoch_to_load = None   # int or None (None means final epoch)\n",
    "\n",
    "list_of_subspace_dim_d = np.arange(1, 16, dtype=int)  # stop one dim below dim n\n",
    "\n",
    "# 0 - Settings for dataset gen\n",
    "criterion = nn.MSELoss()\n",
    "seed_dataset = 7 # (note: seed 4 was used in init training run)\n",
    "\n",
    "################################################################################\n",
    "# params to build data\n",
    "################################################################################\n",
    "# parameters that control dataset size / variety\n",
    "num_W_in_dataset = 1000  #100\n",
    "context_examples_per_W = 1\n",
    "test_ratio = None                # if None, all data samples will be in 'train' bucket (test is None)\n",
    "style_origin_subspace = True\n",
    "style_corruption_orthog = False\n",
    "batch_size = num_W_in_dataset    # full batch\n",
    "\n",
    "#context_lengths_to_eval = None  # if None, will only use the context length from the runinfo file\n",
    "#context_lengths_to_eval = [30, 50, 100, 200, 500]\n",
    "context_lengths_to_eval = [30, 40, 50, 60, 80, 100, 200, 500]\n",
    "\n",
    "# Perform inference on the loaded model (use a trained and untrained model from model directory)\n",
    "for idx_model, model_str in enumerate(models_to_load):\n",
    "    dir_run = model_dirs[idx_model]\n",
    "\n",
    "    datadict_eval_loss[model_str] = dict()\n",
    "    subdict = datadict_eval_loss[model_str]\n",
    "    \n",
    "    # 1A - load the runinfo\n",
    "    runinfo_dict = load_runinfo_from_rundir(dir_run)\n",
    "    loaded_dim_n = runinfo_dict['dim_n']\n",
    "    nn_model_str = runinfo_dict['model']\n",
    "    epochs = runinfo_dict['epochs']\n",
    "    loaded_dim_d = runinfo_dict['style_subspace_dimensions']\n",
    "    sigma2_pure_context = runinfo_dict['sigma2_pure_context']\n",
    "    sigma2_corruption = runinfo_dict['sigma2_corruption']\n",
    "    style_corruption_orthog = runinfo_dict['style_corruption_orthog']\n",
    "    style_origin_subspace = runinfo_dict['style_origin_subspace']\n",
    "    full_loss_sample_interval = runinfo_dict['full_loss_sample_interval']\n",
    "    optimizer_choice = runinfo_dict['optimizer_choice']\n",
    "    context_length = runinfo_dict['context_len']\n",
    "\n",
    "    if context_lengths_to_eval is None:\n",
    "        context_lengths_to_eval = [context_length]\n",
    "\n",
    "    subdict['loaded_dim_d'] = loaded_dim_d\n",
    "    subdict['loaded_dim_n'] = loaded_dim_n\n",
    "\n",
    "    # debugging override\n",
    "    #context_length = 30  # 30= is smallest context len that satisfies internal asserts (somewhat arbitrary)\n",
    "\n",
    "    # compute gamma_star from runinfo dict parameters\n",
    "    gamma_star_val = gamma_star(sigma2_pure_context, sigma2_corruption)\n",
    "\n",
    "    # 1B - load the model\n",
    "    model_fname = runinfo_dict['fname']\n",
    "    nn_model = runinfo_dict['model']\n",
    "    LOADED_NETWORK = load_model_from_rundir(dir_run, epoch_int=epoch_to_load)  # load final epoch weights\n",
    "    # for debugging, load the untrained model\n",
    "    LOADED_NETWORK_UNTRAINED = load_model_from_rundir(dir_run, epoch_int=0)  # load init epoch (0) weights\n",
    "\n",
    "    # 1C - load the baselines from data_for_replot dir\n",
    "    dir_replot = dir_run + os.sep + 'data_for_replot'\n",
    "    lossbaseline_dict = reload_lossbaseline_dict(dir_replot)\n",
    "\n",
    "    # 1D - Load the dataset used during training\n",
    "    loaded_x_train, loaded_y_train, loaded_x_test, loaded_y_test = load_dataset(\n",
    "        dir_run + os.sep + 'training_dataset_split.npz', as_torch=True)\n",
    "    loaded_trainloader = DataLoader(DatasetWrapper(loaded_x_train, loaded_y_train), batch_size=batch_size, shuffle=True)\n",
    "    loaded_testloader = DataLoader(DatasetWrapper(loaded_x_test, loaded_y_test), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # 2) Populate the dictionary: datadict_eval_loss\n",
    "    # - fill values for 'train' key\n",
    "    subdict['trained'] = dict()\n",
    "    # - fill values for 'untrained' key\n",
    "    subdict['untrained'] = dict()\n",
    "    # - then fill values for each evaluated dim d (as an in-context learning demo)\n",
    "    \n",
    "    subdict['trained']['vals_loss_predict_zero'] = lossbaseline_dict['dumb_A_mse_on_train']\n",
    "    subdict['trained']['vals_loss_theory_linalg'] = lossbaseline_dict['heuristic_mse_on_train']\n",
    "    subdict['trained']['vals_loss_theory_linalg_shrunken'] = lossbaseline_dict['heuristic_mse_shrunken_on_train']\n",
    "    subdict['trained']['eq_expected_error_linalg_shrunken'] = theory_linear_expected_error(\n",
    "        loaded_dim_n, loaded_dim_d, sigma2_corruption, sigma2_pure_context)\n",
    "    subdict['trained']['vals_loss_predict_zero'] = lossbaseline_dict['dumb_A_mse_on_train']\n",
    "\n",
    "    with open(dir_replot + os.sep + 'loss_vals_dict.pkl', 'rb') as handle:\n",
    "        loss_vals_dict = pickle.load(handle)\n",
    "    subdict['trained']['loss_vals_dict'] = loss_vals_dict\n",
    "\n",
    "    subdict['trained']['reload_train_and_eval'] = report_dataset_loss(LOADED_NETWORK, criterion, loaded_trainloader, 'RELOAD train', print_val=True)\n",
    "    subdict['trained']['reload_test_and_eval'] = report_dataset_loss(LOADED_NETWORK, criterion, loaded_testloader,    'RELOAD test', print_val=True)\n",
    "\n",
    "\n",
    "    def gen_dataset_given_dim_d(dim_d_inference, context_len_select, seed_dataset):\n",
    "\n",
    "        # Note: test_ratio is None, so all data samples will be in 'train' bucket (foo_test are all None)\n",
    "        X_train, Y_train, X_test, Y_test, train_data_subspaces, test_data_subspaces = data_train_test_split_linear(\n",
    "            context_len=context_len_select,\n",
    "            dim_n=loaded_dim_n,\n",
    "            num_W_in_dataset=num_W_in_dataset,\n",
    "            context_examples_per_W=context_examples_per_W,\n",
    "            samples_per_context_example=1,\n",
    "            test_ratio=None,\n",
    "            style_subspace_dimensions=dim_d_inference,\n",
    "            style_corruption_orthog=style_corruption_orthog,\n",
    "            style_origin_subspace=style_origin_subspace,\n",
    "            sigma2_corruption=sigma2_corruption,\n",
    "            sigma2_pure_context=sigma2_pure_context,\n",
    "            verbose=True,\n",
    "            savez_fname=None,\n",
    "            seed=seed_dataset)\n",
    "\n",
    "        # specify training and testing datasets\n",
    "        subspaces_d_dataset = DatasetWrapper(X_train, Y_train)\n",
    "        #train_dataset.plot()\n",
    "        print('dataset.x.shape', subspaces_d_dataset.x.shape)\n",
    "\n",
    "        assert subspaces_d_dataset.__len__() % batch_size == 0  # want clean multiples to avoid overweight last batch\n",
    "\n",
    "        # Setup data batching\n",
    "        nwork = 0\n",
    "        dataloader_subd = DataLoader(subspaces_d_dataset, batch_size=batch_size, shuffle=True, num_workers=nwork)\n",
    "\n",
    "        return X_train, Y_train, train_data_subspaces, dataloader_subd\n",
    "\n",
    "    # next: fill in these empty arrays\n",
    "    # - for each context length, measure the loss at each value of dim d\n",
    "    for lval in context_lengths_to_eval:\n",
    "        subdict['trained'][lval] = dict()\n",
    "        subdict['untrained'][lval] = dict()\n",
    "        \n",
    "        subdict['trained'][lval]['loss_dim_d_inference'] = np.zeros(len(list_of_subspace_dim_d))\n",
    "        subdict['untrained'][lval]['loss_dim_d_inference'] = np.zeros(len(list_of_subspace_dim_d))\n",
    "    \n",
    "        datadict_eval_loss['linalg_proj'][lval] = np.zeros(len(list_of_subspace_dim_d))\n",
    "        datadict_eval_loss['linalg_proj_shrunken'][lval] = np.zeros(len(list_of_subspace_dim_d))\n",
    "        datadict_eval_loss['predict_zero'][lval] = np.zeros(len(list_of_subspace_dim_d))\n",
    "    \n",
    "        for idx, dim_d_inference in enumerate(list_of_subspace_dim_d):\n",
    "    \n",
    "            # 2A) create dataset without saving\n",
    "            print('\\nCreating inference dataset for dim_d_inference=%d...' % dim_d_inference)\n",
    "            X_train, Y_train, train_data_subspaces, dataloader = gen_dataset_given_dim_d(dim_d_inference, lval, seed_dataset)\n",
    "            assert len(Y_train) == num_W_in_dataset\n",
    "    \n",
    "            # 2B) report dataset loss on the dim d dataset (full)\n",
    "            print('...checking performance for dim_d_inference=%d...' % dim_d_inference)\n",
    "            loss_eval_trained = report_dataset_loss(LOADED_NETWORK, criterion, dataloader,\n",
    "                                                    'trained model (inferenceÃ¥)', print_val=True)\n",
    "            subdict['trained'][lval]['loss_dim_d_inference'][idx] = loss_eval_trained\n",
    "            # now repeat for the untrained model\n",
    "            loss_eval_untrained = report_dataset_loss(LOADED_NETWORK_UNTRAINED, criterion, dataloader,\n",
    "                                                    'not-trained model (inference)',print_val=True)\n",
    "            subdict['untrained'][lval]['loss_dim_d_inference'][idx] = loss_eval_untrained\n",
    "            \n",
    "            # 2C) add extra baselines specific to the dataset\n",
    "            # baseline: linalg_proj\n",
    "            datadict_eval_loss['linalg_proj'][lval][idx] = loss_if_predict_linalg(\n",
    "                criterion, dataloader, 'DATALABEL')\n",
    "            # baseline: linalg_proj_shrunken\n",
    "            datadict_eval_loss['linalg_proj_shrunken'][lval][idx] = loss_if_predict_linalg_shrunken(\n",
    "                criterion, dataloader, 'DATALABEL', sigma2_pure_context, sigma2_corruption, \n",
    "                style_origin_subspace=style_origin_subspace, style_corruption_orthog=style_corruption_orthog)\n",
    "            # baseline: predict_zero\n",
    "            datadict_eval_loss['predict_zero'][lval][idx] = loss_if_predict_zero(\n",
    "                criterion, dataloader, 'DATALABEL')\n",
    "\n",
    "# ========================================================\n",
    "# PLOTTING - Build the primary plot\n",
    "# ========================================================\n",
    "plt.figure(figsize=(6,4))\n",
    "# main curves\n",
    "\n",
    "# Perform inference on the loaded model (use a trained and untrained model from model directory)\n",
    "for idx_model, model_str in enumerate(models_to_load):\n",
    "    dir_run = model_dirs[idx_model]\n",
    "    subdict = datadict_eval_loss[model_str]\n",
    "    suffix = model_labels[idx_model]\n",
    "\n",
    "    # main curves\n",
    "    for lval in context_lengths_to_eval:\n",
    "        plt.plot(list_of_subspace_dim_d, subdict['trained'][lval]['loss_dim_d_inference'],\n",
    "                 '-o', label='inference L=%d (trained %s)' % (lval, suffix), color='black')\n",
    "        plt.plot(list_of_subspace_dim_d, subdict['untrained'][lval]['loss_dim_d_inference'],\n",
    "                 '-o', label='inference L=%d (untrained %s)' % (lval, suffix), color='black', alpha=0.5, markerfacecolor='white')\n",
    "\n",
    "    # baselines - plot trained model baseline information\n",
    "    plt.axhline(subdict['trained']['vals_loss_predict_zero'], label=r'predict $0$',\n",
    "                linestyle='--', alpha=0.75, linewidth=1.5, color='grey')\n",
    "    plt.axhline(subdict['trained']['vals_loss_theory_linalg'], label=r'$P \\tilde x$',\n",
    "                linestyle='-', alpha=0.75, linewidth=1.5, color=COLOR_PROJ)\n",
    "    plt.axhline(subdict['trained']['vals_loss_theory_linalg_shrunken'], label=r'$\\gamma P \\tilde x$',\n",
    "                linestyle='-', alpha=0.75, linewidth=1.5, color=COLOR_PROJSHRUNK)\n",
    "    plt.axhline(subdict['trained']['eq_expected_error_linalg_shrunken'], label=r'Expected error at $\\theta^*$',\n",
    "                linestyle=':', alpha=0.75, linewidth=3, color=COLOR_PROJSHRUNK)\n",
    "    plt.axvline(subdict['loaded_dim_d'], label=r'$d$ (%s)' % suffix,\n",
    "                linestyle='-', alpha=0.75, linewidth=1.5, color=COLOR_TRAIN)\n",
    "\n",
    "    # plot final MSE of trained model (train/test set loss)\n",
    "    plt.axhline(subdict['trained']['loss_vals_dict']['loss_train_interval']['y'][-1],\n",
    "                label=r'Final MSE train (%s)' % suffix,\n",
    "                linestyle='-', alpha=1, linewidth=1.5, color=COLOR_TRAIN)\n",
    "    plt.axhline(subdict['trained']['loss_vals_dict']['loss_test_interval']['y'][-1],\n",
    "                label=r'Final MSE test (%s)' % suffix,\n",
    "                linestyle='-', alpha=1, linewidth=1.5, color=COLOR_TEST)\n",
    "\n",
    "    # baselines - debugging (check MSE on reloaded train dataset)\n",
    "    plt.axhline(subdict['trained']['reload_train_and_eval'],\n",
    "                label=r'RELOAD MSE train (%s)' % suffix,\n",
    "                linestyle='-', alpha=1, linewidth=4.5, color=COLOR_TRAIN, zorder=11)\n",
    "    plt.axhline(subdict['trained']['reload_test_and_eval'],\n",
    "                label=r'RELOAD MSE test (%s)' % suffix,\n",
    "                linestyle='-', alpha=1, linewidth=4.5, color=COLOR_TEST, zorder=11)\n",
    "\n",
    "plt.xlabel(r'$d$ (subspace dim)')\n",
    "plt.ylabel(r'$\\frac{1}{n}$ MSE')\n",
    "plt.title(r'Inference with lower and higher subspace dimensions than training set')\n",
    "\n",
    "plt.legend(ncol=2)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(DIR_OUT + os.sep + 'inference_varying_dim_d.png')\n",
    "plt.savefig(DIR_OUT + os.sep + 'inference_varying_dim_d.svg')\n",
    "plt.show()\n"
   ],
   "id": "759b6b871dbb8be2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Script is above. Alt plots below",
   "id": "82614242d3f2de49"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "158bb9b60bc0c43d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ========================================================\n",
    "# PLOTTING - Build the primary plot\n",
    "# ========================================================\n",
    "context_len_to_marker = ['^', 'd', 'o', 'o', 'o', 'o', 'o', 'o']\n",
    "\n",
    "context_len_to_color = [COLOR_TEST, COLOR_TEST_DARK1, COLOR_TEST_DARK2] + [COLOR_TEST_DARK2] * 5\n",
    "\n",
    "flag_mult_by_n = True        # just for this plot to get nicer scalings\n",
    "flag_plot_baselines = False  # original: True\n",
    "\n",
    "assert context_lengths_to_eval == [30, 40, 50, 60, 80, 100, 200, 500]  # dummy check for hardcoded usage in next line\n",
    "subset_context_lengths_to_eval = [context_lengths_to_eval[a] for a in [0, 2, 7]]\n",
    "\n",
    "# plot main curves\n",
    "plt.figure(figsize=(3.25, 4))\n",
    "\n",
    "ms = 4\n",
    "\n",
    "arr_subspace_dim_d = np.array(list_of_subspace_dim_d)\n",
    "\n",
    "# Perform inference on the loaded model (use a trained and untrained model from model directory)\n",
    "for idx_model, model_str in enumerate(models_to_load):\n",
    "    \n",
    "    dir_run = model_dirs[idx_model]\n",
    "    subdict = datadict_eval_loss[model_str]\n",
    "    suffix = model_labels[idx_model]\n",
    "    \n",
    "    loaded_dim_d = subdict['loaded_dim_d']    \n",
    "    loaded_dim_n = subdict['loaded_dim_n']\n",
    "\n",
    "    if flag_mult_by_n:\n",
    "        scaling = loaded_dim_n\n",
    "    else: \n",
    "        scaling = 1  # i.e. will have no effect\n",
    "    \n",
    "    # main curves\n",
    "    for idx, lval in enumerate(subset_context_lengths_to_eval):\n",
    "\n",
    "        marker_choice = context_len_to_marker[idx]\n",
    "        print(idx_model, marker_choice)\n",
    "\n",
    "        # plot **inference** loss for trained models\n",
    "        plt.plot(list_of_subspace_dim_d, subdict['trained'][lval]['loss_dim_d_inference'] / arr_subspace_dim_d * scaling,\n",
    "                 '-', label='inference L=%d (trained %s)' % (lval, suffix),\n",
    "                 zorder=12, \n",
    "                 marker=marker_choice,\n",
    "                 #color=COLOR_TEST_DARK2, markerfacecolor='white', markersize=ms)\n",
    "                 color=context_len_to_color[idx], \n",
    "                 markeredgecolor='k', markeredgewidth=0.5, \n",
    "                 alpha=0.9, markersize=ms)#, markerfacecolor='white', )\n",
    "\n",
    "        # plot **inference** loss for ---untrained--- models\n",
    "        plt.plot(list_of_subspace_dim_d, subdict['untrained'][lval]['loss_dim_d_inference'] / arr_subspace_dim_d * scaling,\n",
    "                 '-', zorder=11, \n",
    "                 label='inference L=%d (untrained %s)' % (lval, suffix),\n",
    "                 marker=marker_choice, markeredgecolor='k', markeredgewidth=0.5, \n",
    "                 color='grey', alpha=0.5, markersize=ms)#, markerfacecolor='white', )\n",
    "\n",
    "        if idx_model == 0 and flag_plot_baselines:\n",
    "            # do not need to plot the curves below more than once, but don't want extra loop for now\n",
    "            plt.plot(list_of_subspace_dim_d, datadict_eval_loss['linalg_proj'][lval] / arr_subspace_dim_d * scaling,\n",
    "                     '-', label='baseline: proj L=%d' % (lval),\n",
    "                     marker=marker_choice, markeredgecolor='k', markeredgewidth=0.5, \n",
    "                     color=COLOR_PROJ, alpha=0.5, markersize=ms)#, markerfacecolor='white', )\n",
    "            # skip - linalg_proj_shrunken\n",
    "            plt.plot(list_of_subspace_dim_d, datadict_eval_loss['linalg_proj_shrunken'][lval] / arr_subspace_dim_d * scaling,\n",
    "                     '-', label='baseline: proj-shrunk L=%d' % (lval),\n",
    "                     marker=marker_choice, markeredgecolor='k', markeredgewidth=0.5, \n",
    "                     color=COLOR_PROJSHRUNK, alpha=0.5, markerfacecolor=COLOR_PROJSHRUNK, markersize=ms)\n",
    "     \n",
    "    # plot final MSE of trained model (train/test set loss)\n",
    "    loaded_model_final_loss_train = subdict['trained']['loss_vals_dict']['loss_train_interval']['y'][-1] / loaded_dim_d * scaling\n",
    "    loaded_model_final_loss_test  = subdict['trained']['loss_vals_dict']['loss_test_interval']['y'][-1]  / loaded_dim_d * scaling\n",
    "    plt.scatter(loaded_dim_d, loaded_model_final_loss_train, color=COLOR_TRAIN, ec='k', marker='o', s=30, zorder=15)\n",
    "    plt.scatter(loaded_dim_d, loaded_model_final_loss_test, color=COLOR_TEST, ec='k', marker='o', s=30, zorder=15)\n",
    "\n",
    "    loaded_model_init_loss_train = subdict['trained']['loss_vals_dict']['loss_train_interval']['y'][0] / loaded_dim_d * scaling\n",
    "    loaded_model_init_loss_test  = subdict['trained']['loss_vals_dict']['loss_test_interval']['y'][0]  / loaded_dim_d * scaling\n",
    "    plt.scatter(loaded_dim_d, loaded_model_init_loss_train, color='grey', ec='k', marker='o', s=30, zorder=15)\n",
    "\n",
    "    # vert line showing dim-d where model was trained\n",
    "    plt.axvline(subdict['loaded_dim_d'], label=r'$d$ (%s)' % suffix,\n",
    "                linestyle='--', alpha=0.75, linewidth=1.5, color=COLOR_TRAIN)\n",
    "\n",
    "# BASELINE - linear MMSE estimator\n",
    "dense_dspace = np.linspace(0.1, 16, 100)\n",
    "eq_theory_baseline_dense_dspace = [\n",
    "    theory_linear_expected_error(16, DD, sigma2_corruption, sigma2_pure_context) / float(DD) * scaling for DD in dense_dspace\n",
    "]\n",
    "\n",
    "# BASELINE - predict 0 -> E[xx.T] = d * sigma_z^2 \n",
    "dense_dspace = np.linspace(0, 2 * loaded_dim_n, 100)\n",
    "eq_predict_0_baseline_dense_dspace = sigma2_pure_context / loaded_dim_n * scaling * np.ones_like(dense_dspace)\n",
    "\n",
    "plt.plot(dense_dspace, eq_theory_baseline_dense_dspace, label=r'Expected error at $\\theta^*$',\n",
    "         color=COLOR_PROJSHRUNK, linestyle=':', alpha=0.99, linewidth=2)\n",
    "#plt.axhline(eq_theory_baseline_dense_dspace[0], label='Expected error at $\\theta^*$',\n",
    "#            color=COLOR_PROJSHRUNK, linestyle='-', alpha=0.99, linewidth=3)\n",
    "plt.plot(dense_dspace, eq_predict_0_baseline_dense_dspace, label='Predict 0', \n",
    "         color='grey', linestyle='--', alpha=0.99, linewidth=2)\n",
    "\n",
    "\n",
    "plt.xlabel(r'$d$ (subspace dim)')\n",
    "if flag_mult_by_n:\n",
    "    plt.ylabel(r'$d^{-1}$ MSE')\n",
    "else:\n",
    "    plt.ylabel(r'$d^{-1}$ $n^{-1}$ MSE')\n",
    "\n",
    "#plt.legend(fontsize=6)\n",
    "plt.title(r'Inference with lower and higher subspace dimensions than training set')\n",
    "#plt.legend(ncol=2)\n",
    "\n",
    "# remove right and top spine\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "\n",
    "plt.xlim(-0.5, loaded_dim_n+0.3)\n",
    "plt.ylim(0.0, 2.24)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(DIR_OUT + os.sep + 'inference_varying_dim_d_normbyd.png')\n",
    "plt.savefig(DIR_OUT + os.sep + 'inference_varying_dim_d_normbyd.svg')\n",
    "plt.show()\n",
    "plt.show()"
   ],
   "id": "94254ab39de9f39f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e754fc0de357f97f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
